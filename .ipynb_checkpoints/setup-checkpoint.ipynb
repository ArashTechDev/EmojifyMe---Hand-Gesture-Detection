{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "015db509-c923-4971-a65f-f771f969409d",
   "metadata": {},
   "source": [
    "# EMOJIFYME - HAND GESTURE DETECTION USING OPENCV AND MEDIAPIPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa12db-2a65-41d9-bab2-6c0549c37cb2",
   "metadata": {},
   "source": [
    "### This project requires a python v 3.9 - 3.12 and pip v 20.3+\n",
    "<img src=\"./requiredSystemVersion.png\" alt=\"Image Description\" width=\"500\" height=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5069b9f3-75eb-489e-976f-e52bbbc2529d",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (0.10.18)\n",
      "Requirement already satisfied: absl-py in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from mediapipe) (24.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from mediapipe) (3.9.2)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from mediapipe) (4.25.5)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from jax->mediapipe) (0.5.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from jax->mediapipe) (8.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from matplotlib->mediapipe) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from matplotlib->mediapipe) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from matplotlib->mediapipe) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from matplotlib->mediapipe) (6.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from importlib-metadata>=4.6->jax->mediapipe) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mtoyg\\anaconda3\\envs\\emojifyme\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Add required libraries\n",
    "\n",
    "import cv2\n",
    "!pip install mediapipe\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25506ccd-6155-4c8e-b1fe-531dbddd110d",
   "metadata": {},
   "source": [
    "## We're going to be using mediapipe to get 21 landmarks to get data points that will help us in gesture recognition\n",
    "<img src=\"./21Landmarks.png\" alt=\"Image Description\" width=\"500\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aecbbb-0ad4-469e-8ea3-0664de720559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2260a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Webcam Feed', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ecbd7b-a103-4fc8-bccd-bbe456a9d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports (again for Jupyter issues) \n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Drawing utilities\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Convert the image to RGB\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image and find hands\n",
    "    result = hands.process(img_rgb)\n",
    "\n",
    "    # Check if hands are detected\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Draw landmarks on the frame\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Hand Landmarks', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d959f13b-2219-4944-9efb-334ae21a595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_finger_state(hand_landmarks):\n",
    "    \"\"\"\n",
    "    Determine if each finger is extended or not\n",
    "    Returns a list of 5 booleans [thumb, index, middle, ring, pinky]\n",
    "    \"\"\"\n",
    "    # Points for each finger [tip, pip]\n",
    "    finger_points = [\n",
    "        [4, 2],    # Thumb\n",
    "        [8, 6],    # Index\n",
    "        [12, 10],  # Middle\n",
    "        [16, 14],  # Ring\n",
    "        [20, 18]   # Pinky\n",
    "    ]\n",
    "    \n",
    "    finger_states = []\n",
    "    \n",
    "    # Special case for thumb\n",
    "    thumb_tip = hand_landmarks.landmark[4]\n",
    "    thumb_mcp = hand_landmarks.landmark[2]\n",
    "    # Thumb is up if tip is more to the left than mcp\n",
    "    finger_states.append(thumb_tip.x < thumb_mcp.x)\n",
    "    \n",
    "    # For other fingers\n",
    "    for tip_idx, pip_idx in finger_points[1:]:\n",
    "        tip = hand_landmarks.landmark[tip_idx]\n",
    "        pip = hand_landmarks.landmark[pip_idx]\n",
    "        # Finger is considered up if tip is higher than pip\n",
    "        finger_states.append(tip.y < pip.y)\n",
    "    \n",
    "    return finger_states\n",
    "\n",
    "def recognize_gesture(finger_states):\n",
    "    \"\"\"\n",
    "    Recognize specific gestures based on finger states\n",
    "    \"\"\"\n",
    "    if finger_states == [True, False, False, False, False]:\n",
    "        return \"THUMBS_UP\"\n",
    "    elif finger_states == [False, True, True, False, False]:\n",
    "        return \"PEACE\"\n",
    "    elif finger_states == [False, True, False, False, False]:\n",
    "        return \"ONE\"\n",
    "    elif all(finger_states):\n",
    "        return \"HIGH_FIVE\"\n",
    "    elif not any(finger_states):\n",
    "        return \"FIST\"\n",
    "    return \"UNKNOWN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48718624-caa7-4c30-8414-869692e91f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports (again for Jupyter issues)\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Drawing utilities\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Convert the image to RGB\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image and find hands\n",
    "    result = hands.process(img_rgb)\n",
    "\n",
    "    # Check if hands are detected\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Draw landmarks\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # NEW CODE HERE: Get finger states and recognize gesture\n",
    "            finger_states = get_finger_state(hand_landmarks)\n",
    "            gesture = recognize_gesture(finger_states)\n",
    "            \n",
    "            # NEW CODE HERE: Display gesture name\n",
    "            cv2.putText(frame, f\"Gesture: {gesture}\", (10, 30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Hand Gesture Recognition', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d092e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a6cce-3a36-4dcf-84b8-0d2d2cacd860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
